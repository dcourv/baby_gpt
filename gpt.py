import torch
import torch.nn as nn
import torch.nn.functional as F

BATCH_SIZE = 32
BLOCK_SIZE = 8
EMBEDDING_DIM = 128
N_HEADS = 2
TRAIN_SPLIT = 0.8
LEARNING_RATE = 1e-3
TRAINING_ITERS = 5000
EVAL_INTERVAL = 500
EVAL_ITERS = 200
FFN_HIDDEN_LAYER_SIZE = 4 * EMBEDDING_DIM

assert EMBEDDING_DIM % N_HEADS == 0, "The number of heads must evenly divide the model dimension"

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

## Open input data file
with open('input.txt', 'r') as file:
    input_text = file.read()

## Tokenize input text (character-level tokenization)
# @TODO use BPE? (tiktoken?)

chars = sorted(list(set(input_text)))
vocab_size = len(chars)
ctoi = { c: i for i, c in enumerate(chars) }
itoc = { i: c for i, c in enumerate(chars) }

for c in chars:
    assert(itoc[ctoi[c]] == c)

encode = lambda string: [ ctoi[c] for c in string ]
decode = lambda tokens: ''.join([ itoc[t] for t in tokens ])

input_toks = encode(input_text)

# @TODO data will contain high-dim embeddings instead of tokens?

data = torch.tensor(input_toks)

## Train/Val split

train_split_idx = int(TRAIN_SPLIT * len(data))
train_data = data[:train_split_idx]
val_data = data[train_split_idx:]

## Generate training examples

def get_batch(split: str):

    data = train_data if split == 'train' else val_data

    # @NOTE highest possible offset generated by randint is `len(train_data) - BLOCK_SIZE - 1`
    # so last possible training example is
    # x = train_data[len(train_data) - BLOCK_SIZE - 1 : len(train_data) - 1]
    # y = train_data[len(train_data) - BLOCK_SIZE : len(train_data)]
    # (we are within bounds!)
    data_offsets = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))

    x = torch.stack([ data[offset:offset+BLOCK_SIZE] for offset in data_offsets ])
    y = torch.stack([ data[offset+1:offset+BLOCK_SIZE+1] for offset in data_offsets ])

    x, y = x.to(device), y.to(device)

    return x, y

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(EVAL_ITERS)
        for k in range(EVAL_ITERS):
            x, y = get_batch(split)
            logits, loss = model(x, y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    # @NOTE Don't forget to put model back into training mode!
    model.train()
    return out

## Define model

class SelfAttentionHead(nn.Module):

    # @NOTE do we need to make head_size a hyperparameter? Do we need to pass it around?
    def __init__(self, head_size):
        super().__init__()
        # @TODO determine whether we need to explicitly turn off bias
        self.key = nn.Linear(EMBEDDING_DIM, head_size)
        self.query = nn.Linear(EMBEDDING_DIM, head_size)
        self.value = nn.Linear(EMBEDDING_DIM, head_size)
    
    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x) # (B, T, H)
        q = self.query(x) # (B, T, H)
        v = self.value(x) # (B, T, H)

        H = k.shape[-1] # = head_size

        attn_preweights = q @ torch.transpose(k, -1, -2) * (H ** -0.5) # (B, T, T)

        lower_tri = torch.tril(torch.ones(T, T, device=device))
        causal_attn_preweights = attn_preweights.masked_fill(lower_tri == 0, float('-inf'))
        causal_attn_weights = F.softmax(causal_attn_preweights, dim=-1) # (T)

        out = causal_attn_weights @ v # (B, T, H)
        return out
    

class MultiHeadAttention(nn.Module):
    def __init__(self, n_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([ SelfAttentionHead(head_size) for _ in range(n_heads) ])

    def forward(self, x):
        return torch.concat([ head(x) for head in self.heads ], dim=-1) # (B, T, C)
    

class FeedForward(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(EMBEDDING_DIM, FFN_HIDDEN_LAYER_SIZE),
            nn.ReLU(),
            nn.Linear(FFN_HIDDEN_LAYER_SIZE, EMBEDDING_DIM)
        )
    
    def forward(self, x):
        return self.net(x)


class TransformerLayer(nn.Module):
    def __init__(self, n_heads, head_size):
        super().__init__()
        self.ln1 = nn.LayerNorm(EMBEDDING_DIM)
        self.mha = MultiHeadAttention(n_heads, head_size)
        self.ln2 = nn.LayerNorm(EMBEDDING_DIM)
        self.ffn = FeedForward()
    
    def forward(self, x):
        x = x + self.mha(self.ln1(x))
        x = x + self.ffn(self.ln2(x))
        return x


class BabyTransformer(nn.Module):

    def __init__(self):
        super().__init__()
        # @NOTE this is not really an embedding, it is a mapping of every token
        # to logits of the next predicted token
        self.tok_embedding_table = nn.Embedding(vocab_size, EMBEDDING_DIM)
        self.pos_embedding_table = nn.Embedding(BLOCK_SIZE, EMBEDDING_DIM)

        # @NOTE using entire EMBEDDING_DIM for now, head size will be smaller
        self.transformer_layers = nn.Sequential(
            TransformerLayer(N_HEADS, EMBEDDING_DIM // N_HEADS),
            TransformerLayer(N_HEADS, EMBEDDING_DIM // N_HEADS),
            TransformerLayer(N_HEADS, EMBEDDING_DIM // N_HEADS)
        )
        self.layer_norm_final = nn.LayerNorm(EMBEDDING_DIM)
        self.lm_head = nn.Linear(EMBEDDING_DIM, vocab_size)

    def forward(self, batch_toks, batch_tok_targets=None):

        B, T = batch_toks.shape

        tok_embeds = self.tok_embedding_table(batch_toks) # (B, T, C)
        pos_embeds = self.pos_embedding_table(torch.arange(T, device=device)) # (T, C)

        x = tok_embeds + pos_embeds # Broadcasted to (B, T, C)
        # @TODO would `blocks` be a better name than layers?
        x = self.transformer_layers(x) # (B, T, C)
        x = self.layer_norm_final(x)
        logits = self.lm_head(x) # (B, T, V)

        if batch_tok_targets is None:
            loss = None
            return logits, loss

        # prepare logits for use with F.cross_entropy
        # @TODO fix C -> V
        B, T, V = logits.shape
        logits = logits.view(B*T, V)
        targets = batch_tok_targets.view(B*T)
        loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, batch_gened_toks, max_new_toks):
        for _ in range(max_new_toks):
            # Make sure to consider only maximum `BLOCK_SIZE` tokens--otherwise your tensors will
            # be out of range! (Hard to debug on GPU >:()
            batch_context_toks = batch_gened_toks[:, -BLOCK_SIZE:]

            logits, _ = self(batch_context_toks) # (B, T, C)

            # we only care about the last time step (will change when we look at larger context window)
            logits = logits[:, -1, :] # (B, 1, C)

            probs = F.softmax(logits, dim=-1) # (B, C)

            batch_next_toks = torch.multinomial(probs, num_samples=1)

            # add new tokens for prediction
            batch_gened_toks = torch.cat((batch_gened_toks, batch_next_toks), dim=1) # (B, T+1)
        return batch_gened_toks


## Instantiate and train model

model = BabyTransformer()
model = model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

for iter in range(TRAINING_ITERS):

    if iter % EVAL_INTERVAL == 0:
        losses = estimate_loss()
        print(f"Step {iter:4d}: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}")

    # Set gradients to None
    # @NOTE `set_to_none=True` may decrease memory footprint
    optimizer.zero_grad(set_to_none=True)

    x_batch, y_batch = get_batch('train')

    # Forward pass
    logits, loss = model(x_batch, y_batch)

    # Backward pass
    loss.backward()

    optimizer.step()


## Use our trained model to generate text

context = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(model.generate(context, max_new_toks=500)[0].tolist()))